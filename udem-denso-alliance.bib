%% Include here references in the BibTeX format
%% Examples:

@INPROCEEDINGS{concept-graphs,
  author={Gu, Qiao and Kuwajerwala, Ali and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and de Melo, Celso Miguel and Tenenbaum, Joshua B. and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning}, 
  year={2024},
  volume={},
  number={},
  pages={5021-5028},
}

@article{cmu-robot,
  title={Adaptive Mobile Manipulation for Articulated Objects In the Open World},
  author={Xiong, Haoyu and Mendonca, Russell and Shaw, Kenneth and Pathak, Deepak},
  journal={arXiv preprint arXiv:2401.14403},
  year={2024}
}

@INPROCEEDINGS{HOV-SG, 
              AUTHOR    = {Abdelrhman Werby AND Chenguang Huang AND Martin Büchner AND Abhinav Valada AND Wolfram Burgard}, 
              TITLE     = {{Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation}}, 
              BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
              YEAR      = {2024}, 
              ADDRESS   = {Delft, Netherlands}, 
              MONTH     = {July}, 
              DOI       = {10.15607/RSS.2024.XX.077} 
          } 

@inproceedings{scene-fun-3d, 
  title = {{SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes}}, 
  author = {Delitzas, Alexandros and Takmaz, Ayca and Tombari, Federico and Sumner, Robert and Pollefeys, Marc and Engelmann, Francis}, 
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  year = {2024}
}


%%%%% Citations for Miguel + Francesco paragraph %%%%%%
@inproceedings{zhu2024living,
	title        = {Living scenes: Multi-object relocalization and reconstruction in changing 3d environments},
	author       = {Zhu, Liyuan and Huang, Shengyu and Schindler, Konrad and Armeni, Iro},
	year         = 2024,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {28014--28024}
}
@article{gorlo2024long,
	title        = {Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs},
	author       = {Gorlo, Nicolas and Schmid, Lukas and Carlone, Luca},
	year         = 2024,
	journal      = {IEEE Robotics and Automation Letters},
	publisher    = {IEEE}
}
@inproceedings{schmid2024khronos,
	title        = {Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments},
	author       = {Lukas Schmid and Marcus Abate and Yun Chang and Luca Carlone},
	year         = 2024,
	booktitle    = {Proc. of Robotics: Science and Systems (RSS)}
}
@inproceedings{qian2023povslam,
	title        = {{POV-SLAM: Probabilistic Object-Level Variational SLAM}},
	author       = {Qian, Jingxing and Chatrath, Veronica and Servos, James and Mavrinac, Aaron and Burgard, Wolfram and Waslander, Steven L. and Schoellig, Angela},
	year         = 2023,
	booktitle    = {2023 Robotics: Science and Systems (RSS)},
	volume       = {},
	number       = {},
	pages        = {},
	doi          = {}
}
@article{doherty2024mac,
	title        = {M{AC}: {G}raph {S}parsification by {M}aximizing {A}lgebraic {C}onnectivity},
	author       = {Doherty, Kevin and Papalia, Alan and Huang, Yewei and Rosen, David and Englot, Brendan and Leonard, John},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2403.19879}
}
@article{adkins2024obvi,
	title        = {ObVi-SLAM: Long-Term Object-Visual SLAM},
	author       = {Adkins, Amanda and Chen, Taijing and Biswas, Joydeep},
	year         = 2024,
	journal      = {IEEE Robotics and Automation Letters},
	publisher    = {IEEE}
}
@inproceedings{vodisch2022continual,
	title        = {Continual slam: Beyond lifelong simultaneous localization and mapping through continual learning},
	author       = {V{\"o}disch, Niclas and Cattaneo, Daniele and Burgard, Wolfram and Valada, Abhinav},
	year         = 2022,
	booktitle    = {The International Symposium of Robotics Research},
	pages        = {19--35},
	organization = {Springer}
}
@inproceedings{schmid2022panoptic,
	title        = {Panoptic multi-{TSDFs}: {A} flexible representation for online multi-resolution volumetric mapping and long-term dynamic scene consistency},
	author       = {Schmid, Lukas and others},
	year         = 2022,
	booktitle    = {C-ICRA}
}
@article{saavedra2025perpetua,
	title        = {Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments},
	author       = {Saavedra-Ruiz, Miguel and Nashed, Samer and Gauthier, Charlie and Paull, Liam},
	year         = 2025,
	journal      = {arXiv preprint},
	url          = {https://montrealrobotics.ca/perpetua/}
}
@inproceedings{krajnik2015waldo,
	title        = {Where's waldo at time t ? using spatio-temporal models for mobile robot search},
	author       = {Krajník, Tomáš and Kulich, Miroslav and Mudrová, Lenka and Ambrus, Rares and Duckett, Tom},
	year         = 2015,
	booktitle    = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
	volume       = {},
	number       = {},
	pages        = {2140--2146},
	doi          = {10.1109/ICRA.2015.7139481},
	keywords     = {Search problems;Mobile robots;Testing;Robot sensing systems;Training;mobile robotics;long-term autonomy}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}
@misc{nvidiaisaacsim,
  author       = {NVIDIA Corporation},
  title        = {NVIDIA Isaac Sim},
  year         = {2024},
  url          = {https://developer.nvidia.com/isaac-sim}
}

@misc{pi0,
  author       = {Physical Intelligence},
  title        = {Pi0: A Vision-Language-Action Flow Model for
General Robot Control},
  year         = {2024},
  url          = {https://www.physicalintelligence.company/blog/pi0}
}


@InProceedings{pmlr-v235-zhou24f,
  title = 	 {{R}obo{D}reamer: Learning Compositional World Models for Robot Imagination},
  author =       {Zhou, Siyuan and Du, Yilun and Chen, Jiaben and Li, Yandong and Yeung, Dit-Yan and Gan, Chuang},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {61885--61896},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhou24f/zhou24f.pdf},
  url = 	 {https://proceedings.mlr.press/v235/zhou24f.html},
  abstract = 	 {Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation. However, one major issue in such models is generalization – models are limited to synthesizing videos subject to language instructions similar to those seen at training time. This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments. To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation. We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos. We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components. We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image. Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation.}
}


% --------------------------------------------------- %


% --- charlie --- %

@article{10.1145/3571730,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {248},
numpages = {38},
keywords = {Hallucination, intrinsic hallucination, extrinsic hallucination, faithfulness in NLG, factuality in NLG, consistency in NLG}
}

@inproceedings{49255,
title	= {On Faithfulness and Factuality in Abstractive Summarization},author	= {Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan Thomas Mcdonald},year	= {2020},booktitle	= {Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics (ACL)}}

@inproceedings{10.1145/3442188.3445922,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{
sharma2024towards,
title={Towards Understanding Sycophancy in Language Models},
author={Mrinank Sharma and Meg Tong and Tomasz Korbak and David Duvenaud and Amanda Askell and Samuel R. Bowman and Esin DURMUS and Zac Hatfield-Dodds and Scott R Johnston and Shauna M Kravec and Timothy Maxwell and Sam McCandlish and Kamal Ndousse and Oliver Rausch and Nicholas Schiefer and Da Yan and Miranda Zhang and Ethan Perez},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tvhaxkMKAn}
}

@inproceedings{
wei2023jailbroken,
title={Jailbroken: How Does {LLM} Safety Training Fail?},
author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=jA235JGM09}
}

@article{robey2024jailbreaking,
    title={Jailbreaking LLM-Controlled Robots},
    author={Robey, Alexander and Ravichandran, Zachary and Kumar, Vijay and Hassani, Hamed and Pappas, George J.},
    journal={arXiv preprint arXiv:2410.13691},
    year={2024}
}


@misc{ji2024aialignmentcomprehensivesurvey,
      title={AI Alignment: A Comprehensive Survey}, 
      author={Jiaming Ji and Tianyi Qiu and Boyuan Chen and Borong Zhang and Hantao Lou and Kaile Wang and Yawen Duan and Zhonghao He and Jiayi Zhou and Zhaowei Zhang and Fanzhi Zeng and Kwan Yee Ng and Juntao Dai and Xuehai Pan and Aidan O'Gara and Yingshan Lei and Hua Xu and Brian Tse and Jie Fu and Stephen McAleer and Yaodong Yang and Yizhou Wang and Song-Chun Zhu and Yike Guo and Wen Gao},
      year={2024},
      eprint={2310.19852},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.19852}, 
}

@book{engelberger2012robotics,
  title={Robotics in Practice: Management and applications of industrial robots},
  author={Engelberger, J.F.},
  isbn={9781468471205},
  url={https://books.google.ca/books?id=r1Z-BgAAQBAJ},
  year={2012},
  publisher={Springer US}
}

@article{roco,
	title        = {RoCo: Dialectic Multi-Robot Collaboration with Large Language Models},
	author       = {Zhao Mandi and Shreeya Jain and Shuran Song},
	year         = 2023,
	journal      = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
	pages        = {286--299},
	url          = {https://api.semanticscholar.org/CorpusID:259501567},
	eprint       = {2307.04738},
	archiveprefix = {arXiv},
	primaryclass = {cs.RO}
}


@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@InProceedings{10.1007/978-3-031-59057-3_25,
author="Amin Mansour, Elham
and Zheng, Hehui
and Katzschmann, Robert K.",
editor="Filipe, Joaquim
and R{\"o}ning, Juha",
title="Fast Point Cloud to Mesh Reconstruction for Deformable Object Tracking",
booktitle="Robotics, Computer Vision and Intelligent Systems",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="391--409",
abstract="The world around us is full of soft objects we perceive and deform with dexterous hand movements. For a robotic hand to control soft objects, it has to acquire online state feedback of the deforming object. While RGB-D cameras can collect occluded point clouds at a rate of 30 Hz, this does not represent a continuously trackable object surface. Hence, in this work, we developed a method that takes as input a template mesh which is the mesh of an object in its non-deformed state and a deformed point cloud of the same object, and then shapes the template mesh such that it matches the deformed point cloud. The reconstruction of meshes from point clouds has long been studied in the field of Computer graphics under 3D reconstruction and 4D reconstruction, however, both lack the speed and generalizability needed for robotics applications. Our model is designed using a point cloud auto-encoder and a Real-NVP architecture. Our trained model can perform mesh reconstruction and tracking at a rate of 58 Hz on a template mesh of 3000 vertices and a deformed point cloud of 5000 points and is generalizable to the deformations of six different object categories which are assumed to be made of soft material in our experiments (scissors, hammer, foam brick, cleanser bottle, orange, and dice). The object meshes are taken from the YCB benchmark dataset. An instance of a downstream application can be the control algorithm for a robotic hand that requires online feedback from the state of the manipulated object which would allow online grasp adaptation in a closed-loop manner. Furthermore, the tracking capacity of our method can help in the system identification of deforming objects in a marker-free approach. In future work, we will extend our trained model to generalize beyond six object categories and additionally to real-world deforming point clouds.",
isbn="978-3-031-59057-3"
}

@InProceedings{Yang_2024_CVPR,
    author    = {Yang, Yue and Sun, Fan-Yun and Weihs, Luca and VanderBilt, Eli and Herrasti, Alvaro and Han, Winson and Wu, Jiajun and Haber, Nick and Krishna, Ranjay and Liu, Lingjie and Callison-Burch, Chris and Yatskar, Mark and Kembhavi, Aniruddha and Clark, Christopher},
    title     = {Holodeck: Language Guided Generation of 3D Embodied AI Environments},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {16227-16237}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}

@article{molmo2024,
  title={Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models},
  author={Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Jen Dumas and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi},
  journal={arXiv preprint arXiv:2409.17146},
  year={2024}
}

@article{ai2thor,
  author={Eric Kolve and Roozbeh Mottaghi and Winson Han and
          Eli VanderBilt and Luca Weihs and Alvaro Herrasti and
          Daniel Gordon and Yuke Zhu and Abhinav Gupta and
          Ali Farhadi},
  title={{AI2-THOR: An Interactive 3D Environment for Visual AI}},
  journal={arXiv},
  year={2017}
}

@article{objaverse,
  title={Objaverse: A Universe of Annotated 3D Objects},
  author={Matt Deitke and Dustin Schwenk and Jordi Salvador and Luca Weihs and
          Oscar Michel and Eli VanderBilt and Ludwig Schmidt and
          Kiana Ehsani and Aniruddha Kembhavi and Ali Farhadi},
  journal={arXiv preprint arXiv:2212.08051},
  year={2022}
}

@inproceedings{kirillov2023segment-SAM,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4015--4026},
  year={2023}
}

@article{murai2024mast3r-slam,
  title={MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors},
  author={Murai, Riku and Dexheimer, Eric and Davison, Andrew J},
  journal={arXiv preprint arXiv:2412.12392},
  year={2024}
}

@inproceedings{leroy2024grounding-mast3r,
  title={Grounding image matching in 3d with mast3r},
  author={Leroy, Vincent and Cabon, Yohann and Revaud, J{\'e}r{\^o}me},
  booktitle={European Conference on Computer Vision},
  pages={71--91},
  year={2024},
  organization={Springer}
}

@article{bai2025qwen2,
  title={Qwen2. 5-vl technical report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}


@article{future-mapping,
  author       = {Andrew J. Davison},
  title        = {FutureMapping: The Computational Structure of Spatial {AI} Systems},
  journal      = {CoRR},
  volume       = {abs/1803.11288},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.11288},
  eprinttype    = {arXiv},
  eprint       = {1803.11288},
  timestamp    = {Mon, 13 Aug 2018 16:45:59 +0200},
}

@inproceedings{3Dscenegraph,
  title = {{3D} Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans},
  author = {Rosinol, A. and Gupta, A. and Abate, M. and Shi, J. and Carlone, L.},
  booktitle = {Robotics: Science and Systems (RSS)},
  year = {2020},
  pdf = {https://arxiv.org/pdf/2002.06289.pdf},
  url = {http://news.mit.edu/2020/robots-spatial-perception-0715},
  video = {https://www.youtube.com/watch?v=SWbofjhyPzI},
  doi = {10.15607/RSS.2020.XVI.079},
}